### 批量归一化（Batch Normalization）

现有问题：损失函数在神经网络的顶部，最上面的几层可以更快的学习到参数，更快地收敛，底部的几层变化较慢，但是底部一变化，所有都要变，最后的几层需要重新学习多次，如何实现学习底部层的时候避免改变顶部层。



#### 批量归一化的思想

![](D:\学习笔记\深度学习\L\Snipaste_2023-11-30_19-40-02.png)

#### 批量归一化层

![](D:\学习笔记\深度学习\L\Snipaste_2023-11-30_19-49-52.png)

可以加速收敛速度，但一般不改变模型精度
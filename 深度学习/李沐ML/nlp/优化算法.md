![](D:\学习笔记\深度学习\L\Snipaste_2024-08-04_19-09-17.jpg)

![](D:\学习笔记\深度学习\L\Snipaste_2024-08-04_19-12-31.jpg)

最小化一个凸损失函数 L(θ，其中 θ是模型参数。在这种情况下，所有使得损失函数值小于等于某一给定值的参数集合 {θ∣L(θ)≤c}将形成一个凸集。

对于集合 C中的任意两点，连接它们的线段上的所有点也都在集合 C中。

![Snipaste_2024-08-04_19-12-37](D:\学习笔记\深度学习\L\Snipaste_2024-08-04_19-12-37.jpg)

在 x 和 y之间的任意点处，函数值不会超过连接 f(x) 和 f(y)) 两点的直线上的值。

![Snipaste_2024-08-04_19-16-14](D:\学习笔记\深度学习\L\Snipaste_2024-08-04_19-16-14.jpg)

![](D:\学习笔记\深度学习\L\Snipaste_2024-08-05_02-17-03.jpg)

![Snipaste_2024-08-05_02-29-28](D:\学习笔记\深度学习\L\Snipaste_2024-08-05_02-29-28.jpg)

![Snipaste_2024-08-05_02-30-12](D:\学习笔记\深度学习\L\Snipaste_2024-08-05_02-30-12.jpg)

**SGD每次迭代中使用一个样本（或一个小批量的样本）来计算梯度**

### 随机梯度下降优点：

**计算效率高**：每次迭代只需计算一个样本的梯度，计算量小。

**内存消耗低**：无需在内存中存储整个数据集，适用于大规模数据集。

**快速收敛**：由于每次迭代使用的样本不同，梯度方向具有随机性，可以帮助模型跳出局部最优解，达到更好的全局最优。

### 随机梯度下降的缺点

**收敛不稳定**：由于每次更新使用的梯度是基于一个样本的，梯度更新方向具有高方差，可能导致收敛过程中的抖动。

**需要较小的学习率**：为了减小抖动，通常需要较小的学习率，**这可能会使收敛速度变慢**。

**需要更多的迭代次数**：由于每次迭代只使用一个样本，可能需要更多的迭代次数才能达到收敛。

![](D:\学习笔记\深度学习\L\Snipaste_2024-08-05_02-34-08.jpg)

![](D:\学习笔记\深度学习\L\Snipaste_2024-08-05_02-51-15.jpg)

![Snipaste_2024-08-05_02-51-50](D:\学习笔记\深度学习\L\Snipaste_2024-08-05_02-51-50.jpg)

![](D:\学习笔记\深度学习\L\Snipaste_2024-08-05_02-57-13.jpg)
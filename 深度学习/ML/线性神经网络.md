### 线性神经网络

一.梯度下降是通过沿着参数梯度的反方向不断更新参数

小批量随机梯度是ML默认算法

批量太小：每次计算量太小，不舍和并行来最大利用资源

批量太大：内存消耗增加，浪费资源

两个重要参数：**学习率和批量大小**

​       如果batch_size不能被len(features)整除，最后一个batch_size会小于10，但是在计算梯度时，还是会计算10个样本的梯度

损失函数一般使用L2 Loss：平方误差函数：nn.MSELoss()   参数更新不够平滑

​                               L1 Loss：绝对值误差函数：l(y, y′) = | y − y′| 0点不可导，-1和+1的震荡

​                               Huber‘s Robust Loss：

蓝色的曲线表示：y=0时，变化预测值y’的函数。
绿色曲线表示：[似然函数](https://so.csdn.net/so/search?q=%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0&spm=1001.2101.3001.7020)。e^-l。 是一个高斯分布。
橙色的线：表示损失函数的梯度

​     当真实值和预测值差得比较远的时候，梯度是个固定值，用一个比较均匀的力度帮你往回拉，在比较靠近的时候，也就是优化末期的时候，梯度的绝对值会变得越来越小，这样可以保证优化是比较平滑的。

![1700447513560](D:\学习笔记\深度学习\L\Snipaste_2023-11-25_14-39-24.png)

二.softmax()多分类问题

用交叉熵损失函数

H(p,q)=−∑ip(i)log(q(i))

其中，i 表示类别的索引，p(i) 是真实分布中类别 i 的概率，q(i) 是模型输出的概率。

H([1,0],[0.8,0.2])=−(1⋅log(0.8)+0⋅log(0.2))
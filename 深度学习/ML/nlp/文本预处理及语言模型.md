### 核心思想

把语言变成能训练的数据

### 词元化

```python
def tokenize(lines, token='word'):  #@save
    """将文本行拆分为单词或字符词元"""
    if token == 'word':
        return [line.split() for line in lines]
    elif token == 'char':
        return [list(line) for line in lines]
    else:
        print('错误：未知词元类型：' + token)

tokens = tokenize(lines)
for i in range(11):
    print(tokens[i])

word：把一行文本以空格为分隔符分成一个一个词
char：把一行文本分成一个一个字符
```

## 词表

把不同token映射到不同索引中

### 语言模型

给定文本序列x1......xT,,目标是估计联合概率。

1.做预训练模型(BERT,GPT3)

2.生成文本，给定前面几个词，不断使用xt ∼ p(xt|x1,…,xt−1)来生成后续文本

3.判断多个序列哪个更常见

**(x, x′)代表一个序列，即两个单词**

![](D:\学习笔记\深度学习\L\Snipaste_2024-07-21_20-28-04.jpg)

![](D:\学习笔记\深度学习\L\Snipaste_2024-07-22_02-04-40.jpg)

二元语法：每一个词概率和前面一个有关

n元语法要存字典中所有长度为n，n-1，n-2....1的序列的概率，比如二元语法中字典长度为1000，则所有长度为2的子序列种类有1000*1000种，要存这1000000个子序列出现的概率。
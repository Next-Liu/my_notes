### 编码器

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-12_10-28-53.png)

#### 1. 输入部分

##### 1.1 embedding

##### 1.2 位置编码

pos代表的是绝对位置信息

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_16-12-51.png)

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_16-14-17.png)

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_16-25-12.png)

相对位置信息

为什么用sin，cos来做位置信息？

假设有两个位置 pos1和 pos2，如果它们之间的距离是 k，那么它们的位置编码之间的关系只取决于 k，而与 pos1和 pos2 的具体值无关。这种性质使得位置编码能够很好地捕捉到序列中元素的相对位置关系

#### 2. 掩码张量

##### 2.1 什么是掩码张量

- 掩代表遮掩，码就是我们张量中的数值，它的尺寸不定，里面一般只有1和0的元素，代表位置被遮掩或者不被遮掩，至于是0位置被遮掩还是1位置被遮掩可以自定义，因此它的作用就是让另外一个张量中的一些数值被遮掩，也可以说被替换, 它的表现形式是一个张量.

##### 2.2 掩码张量的作用

- 在transformer中, 掩码张量的主要作用在应用attention时，**有一些生成的attention张量中的值计算有可能已知了未来信息而得到的**，未来信息被看到是因为训练时会把整个输出结果都一次性进行Embedding，**但是理论上解码器的的输出却不是一次就能产生最终结果的**，而是一次次通过上一次结果综合得出的，因此，未来的信息可能被提前利用. 所以会进行遮掩. 关于解码器的有关知识将在后面的章节中讲解.

  ![](D:\学习笔记\深度学习\L\Snipaste_2024-10-12_16-23-29.png)

#### 3. 注意力机制

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_19-31-14.png)

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_19-32-32.png)

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_19-34-34.png)

Q,K,V:

对一段文本使用关键词进行描述，

整个文本看作query，预先作为提示的关键词看作key， value为看到这段文本脑子复现的信息

key和value基本相同（开始的提示和脑子的思考基本相同），随着注意力作用，value开始变化，根据提示key生成query关键词value的方法

输入一般：key和value相同，与query不同(看到什么就学到什么)

自注意力：key和query和value都相同，用文本自身表达自身，对文本自身做特征提取

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-17_10-09-31.png)

##### 3.1 **如何获取Q,K,V矩阵**

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_19-36-12.png)

##### 3.2 计算Attention Value

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_19-38-44.png)

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_19-40-01.png)



**多头注意力就是有多个Wq，Wk，Wv矩阵，相当于把原始信息放到多个空间，捕捉多个子空间信息**

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_19-39-28.png)

##### 3.3 多头注意力机制

​	每个头开始从词义层面分割输出的张量，也就是**每个头都想获得一组Q，K，V进行注意力机制的计算**，但是句子中的每个词的表示只获得一部分，也就是**只分割了最后一维的词嵌入向量. 这就是所谓的多头**，将每个头的获得的输入送到注意力机制中, 就形成多头注意力机制.

​	![](D:\学习笔记\深度学习\L\Snipaste_2024-10-21_10-06-55.png)

​	作用：这种结构设计能让每个注意力机制去优化每个词汇的不同特征部分，从而均衡同一种注意力机制可能产生的偏差，让词义拥有来自更多元的表达，实验表明可以从而提升模型效果.

#### 4. 前馈神经网络

- 在Transformer中前馈全连接层就是具有两层线性层的全连接网络.
- 前馈全连接层的作用:
  - 考虑注意力机制可能对复杂过程的拟合程度不够, 通过增加两层网络来增强模型的能力.

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_20-55-02.png)

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-22_15-12-17.png)

#### 5. 残差

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_20-27-27.png)



![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_20-29-07.png)

##### 5.1 Batch Normalization vs. Layer Normalization

Batch Normalization在nlp任务一般不用

样本同一维度进行处理，这样是假设样本每一维度代表的含义相同

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_20-37-23.png)

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_20-38-35.png)

##### 5.2 BN的缺点：

1.batch_size较小的时候效果较差

2.在RNN任务效果较差，输入是动态的 对第一个单词做均值方差，对第二个单词做均值方差......

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_20-41-36.png)

#### 6. Layer Normalization

对一个样本的所有单词做缩放

### 解码器

- 由N个解码器层堆叠而成
- 每个解码器层由三个子层连接结构组成
- 第一个子层连接结构包括一个多头自注意力子层和规范化层以及一个残差连接
- 第二个子层连接结构包括一个多头注意力子层和规范化层以及一个残差连接
- 第三个子层连接结构包括一个前馈全连接子层和规范化层以及一个残差连接

作用：每个解码器层根据给定的输入向目标方向进行特征提取操作，即解码过程，根据编码器的结果以及上一次预测的结果, 对下一次可能出现的'值'进行特征表示.

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_20-59-27.png)



![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_21-01-08.png)

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-10_21-04-02.png)

### 输出层

![](D:\学习笔记\深度学习\L\Snipaste_2024-10-22_22-36-44.png)

- 线性层的作用:
  - 通过对上一步的线性变化得到指定维度的输出, 也就是转换维度的作用.
- softmax层的作用:
  - 使最后一维的向量中的数字缩放到0-1的概率值域内, 并满足他们的和为1.

```python
# nn.functional工具包装载了网络层中那些只进行计算, 而没有参数的层
import torch.nn.functional as F

# 将线性层和softmax计算层一起实现, 因为二者的共同目标是生成最后的结构
# 因此把类的名字叫做Generator, 生成器类
class Generator(nn.Module):
    def __init__(self, d_model, vocab_size):
        """初始化函数的输入参数有两个, d_model代表词嵌入维度, vocab_size代表词表大小."""
        super(Generator, self).__init__()
        # 首先就是使用nn中的预定义线性层进行实例化, 得到一个对象self.project等待使用, 
        # 这个线性层的参数有两个, 就是初始化函数传进来的两个参数: d_model, vocab_size
        self.project = nn.Linear(d_model, vocab_size)

    def forward(self, x):
        """前向逻辑函数中输入是上一层的输出张量x"""
        # 在函数中, 首先使用上一步得到的self.project对x进行线性变化, 
        # 然后使用F中已经实现的log_softmax进行的softmax处理.
        # 在这里之所以使用log_softmax是因为和我们这个pytorch版本的损失函数实现有关, 在其他版本中将修复.
        # log_softmax就是对softmax的结果又取了对数, 因为对数函数是单调递增函数, 
        # 因此对最终我们取最大的概率值没有影响. 最后返回结果即可.
        return F.log_softmax(self.project(x), dim=-1)
```

![](D:\学习笔记\深度学习\L\Snipaste_2024-08-01_01-37-44.jpg)
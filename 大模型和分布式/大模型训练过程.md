### 预训练（Pretraining）

​	学习大量无标签文本数据来掌握语言的基本结构和语义规律，并使用一种名为“掩码语言模型”（Masked Language Model, MLM）的方法。这意味着在训练样本中，一些词汇会被随机掩盖，模型需要根据上下文信息预测这些被掩盖的词汇。(RNN、Transformer)

### 监督微调SFT（Supervised Finetuning）

​	模型使用特定任务的标签数据进行训练，学习如何根据输入生成更准确、更相关的回复。

### 奖励建模RM（Reward Modeling）

​	对模型生成的输出进行评分

**收集数据**：收集包含输入和预期输出的示例数据。

**训练奖励模型**：使用这些示例数据训练一个监督学习模型，该模型的目标是预测人类标注者的评分。这个模型可以是一个简单的回归模型，也可以是一个更复杂的神经网络。

### 强化学习RL（Reinforcement Learning）

​	使用奖励模型提供的反馈来进一步优化模型的参数。强化学习通过最大化累积奖励来改进模型的决策过程。

**生成样本**：使用当前的语言模型生成一批样本。

**计算奖励**：使用奖励模型对生成的样本进行评分，计算每个样本的奖励值。

**优化模型**：使用强化学习算法（如策略梯度算法）优化模型的参数，以最大化累积奖励。

### 异常检测算法

##### 统计方法：

​	基于正态分布的方法：假设数据服从正态分布，计算数据点的$z$分数，如果$z$分数大于某个阈值，则认为该数据点是异常点。

```python
z_scores = (x - mean) / std_dev
```

##### 基于距离方法：

​	KNN：对于每个数据点，计算其与其他数据点的距离，然后选取距离最近的$k$个数据点。如果这$k$个数据点的平均距离大于某个阈值，则认为该数据点是异常点。

​	LOF：基于密度的方法，计算数据点的局部密度与其邻居的局部密度之比来检测异常。LOF值越大，表示数据点越可能是异常点。
### url去重：

一个url爬取多次，取最新一次结果

### 相似文档去重：

MinHash**

### 行级别去重：

文档中某一行出现超过一个基数(6次)，去除掉，网页一些提示性内容，和文本没什么关系



### n-gram coverage ratio去除掉有重复内容的行

### 1. 数据预处理

对文本进行标准化处理，移除无关字符如标点符号，并将所有文本转换为小写，以便于后续处理的一致性。

### 2. 生成n-gram

n-gram是从文本中提取的连续n个单词的序列。比如，给定一个文本"this is a sample"，其bi-gram（n=2）为：

- ("this", "is")
- ("is", "a")
- ("a", "sample")

n-gram可以捕捉文本中的局部模式，有助于比较不同文本之间的相似性。

### 3. 计算n-gram覆盖率

n-gram覆盖率指的是一个文本中不同n-gram出现的次数。这可以帮助我们量化和比较不同文本的内容相似度。具体计算步骤如下：

1. 对每个文本生成n-gram。
2. 使用计数器（如Python的`collections.Counter`）计算每个n-gram的出现频率。
3. 将这些频率值存储为特征向量，代表该文本的n-gram分布。

### 4. 去除重复内容的行

通过比较不同文本的n-gram分布，识别和去除重复内容的行。具体过程如下：

1. 初始化一个集合，用于存储已见过的n-gram分布。
2. 遍历所有文本的n-gram分布，如果当前文本的n-gram分布未在集合中出现过，则将其加入集合，并将该文本视为唯一文本。
3. 如果当前文本



dirty words

token-distribution Kullback-Leibler Distance

### 知识分类：

​	对数据进行分类，可以进行数据混合，例如50%通用知识（对大模型更好，小模型一般），25%数学和推理知识，17%代码数据，8%多语言

### 退火数据：

​	训练完后再在数学和代码数据训练一次

​	



